# ðŸ§  LangChain Agent Integrations with Ollama LLaMA 3

This repository showcases various implementations and integrations using **LangChain**, focusing on chat models, prompt templates, chains, Retrieval-Augmented Generation (RAG), and agent-based systems. It leverages **free, open-source models** like **Ollama's LLaMA 3** for all LLM-related functionality.

---

## ðŸš€ Features

- **ðŸ” RAG with Metadata Support**
  - Document loading, chunking, and embedding
  - Vector storage with ChromaDB
  - Query answering based on retrieved context

- **ðŸ› ï¸ Agent Execution with Tools**
  - REACT-style agent integrated with LangChain Tools
  - Example tool: system time retriever

- **ðŸ’¬ LLM Integration (Ollama)**
  - Uses Ollama's local LLaMA 3 model
  - No OpenAI or cloud dependency

- **ðŸ“„ Prompt Templates and Chains**
  - Custom prompts for guided reasoning
  - Chains to link retrieval, formatting, and generation

---

## ðŸ—ï¸ Project Structure

```
lang-agent/
â”œâ”€â”€ documents/                     # Input text files for document-based RAG
â”‚   â”œâ”€â”€ dracula.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ db/                           # Vector DB persistence directories
â”‚   â”œâ”€â”€ chroma_db/                # Basic RAG
â”‚   â””â”€â”€ chroma_db_with_metadata/  # Metadata-aware RAG
â”‚
â”œâ”€â”€ agents/                       # Agent-related code
â”‚   â””â”€â”€ agent_with_tools.py       # REACT agent with custom tools
â”‚
â”œâ”€â”€ rag/                          # RAG scripts
â”‚   â”œâ”€â”€ 1a_basic_part_1.py        # Basic RAG - chunking and vector DB init
â”‚   â”œâ”€â”€ 1b_query_part_2.py        # Basic RAG - query from existing DB
â”‚   â”œâ”€â”€ 2a_rag_basics_metadata.py # RAG with file metadata
â”‚   â””â”€â”€ 2b_rag_query_metadata.py  # Query from metadata-based RAG
â”‚
â”œâ”€â”€ .env                          # Environment variables (optional)
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project documentation
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/vi-bs/lang-agent.git
cd lang-agent
```

### 2. Set Up Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # For Mac/Linux
venv\Scripts\activate     # For Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install Ollama and LLaMA 3

> ðŸ§  Make sure you have Ollama installed. You can get it from: https://ollama.com

Then pull the LLaMA 3 model:

```bash
ollama pull llama3
```

---

## âœ… Running the Examples

### Basic RAG Initialization
```bash
python rag/1a_basic_part_1.py
```
### Querying the RAG Vector Store
```bash
python rag/1b_query_part_2.py
```

### Metadata-Based RAG Initialization
```bash
python rag/2a_rag_basics_metadata.py
```
### Querying with Metadata-Based RAG
```bash
python rag/2b_rag_query_metadata.py
```

### Running the Agent with Tools
```bash
python agents/agent_with_tools.py
```

---

## ðŸ§ª Example Agent Query

> **Input**: "What is the current time in London? (You are in India)"
>
> **Tool Used**: get_system_time

---

## ðŸ“œ License
MIT License

---

## ðŸ™Œ Acknowledgments
- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://ollama.com)
- [Chroma](https://github.com/chroma-core/chroma)
- [LLaMA 3 by Meta](https://ai.meta.com/llama/)

---

## ðŸ’¡ Future Enhancements
- Add support for document upload via UI
- Integrate LangGraph workflows
- Experiment with custom Ollama models and embeddings

